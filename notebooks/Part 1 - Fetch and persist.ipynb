{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the main Wikipedia page\n",
    "main_url = \"https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents\"\n",
    "\n",
    "page = requests.get(main_url).text\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "# Finds all speeches from 1900 to 2021:\n",
    "soup = soup.find_all('li')[144:268]\n",
    "\n",
    "# Initialize empty list for speech URLs:\n",
    "speech_urls = []\n",
    "\n",
    "# Initialize empty list for years - to be used for filenames:\n",
    "year_list = []\n",
    "\n",
    "# Loads all speech URls into list:\n",
    "for li in soup:\n",
    "    \n",
    "    # Finds URL and appends to 'speech_urls' list\n",
    "    url = \"https://en.wikisource.org/\" + li.a.get('href')\n",
    "    speech_urls.append(url)\n",
    "    \n",
    "    # Finds text next to the second <a> tag, as this text contains the date infomation for the speech: \n",
    "    this_date = li.find_all('a')[1].next_sibling\n",
    "    \n",
    "    # We use regex to find the year from the date string:\n",
    "    this_year = re.search(r\"\\b(19|20)\\d{2}\\b\", this_date)\n",
    "    \n",
    "    # Year is appended to 'year_list'\n",
    "    year_list.append(this_year.group(0))\n",
    "    \n",
    "# Creates a dictionary so we can iterate 'year_list' and 'speech_urls' at the same time:\n",
    "url_year_zip = zip(year_list, speech_urls)\n",
    "url_year_dict = dict(url_year_zip)\n",
    "\n",
    "# Opens each URL, reads text and saves text:\n",
    "for year, url in url_year_dict.items():\n",
    "    page = requests.get(url).content\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    # We remove the first and the last <p> tag to clean the data:\n",
    "    text = soup.find_all('p')[1:-1]\n",
    "    \n",
    "    # Sets filename and increments year:\n",
    "    filename = \"./speeches/\" + year + \".txt\" \n",
    "    \n",
    "    # Creates file:\n",
    "    f = open(filename ,\"w+\")\n",
    "    for line in text:\n",
    "        # .getText() method is added to extract content from each <p> tag, so the <p> tag is not included:\n",
    "        f.write(line.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
