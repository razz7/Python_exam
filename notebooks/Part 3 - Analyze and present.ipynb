{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads pickled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_clean = pd.read_pickle('pickled_data/data_first_clean.pkl')\n",
    "df_second_clean = pd.read_pickle('pickled_data/data_second_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis on Polarity and Subjectivity for all speeches using TextBlob:\n",
    "def sentiment(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "df_first_clean['Polarity'] = df_first_clean['first_clean'].apply(sentiment).apply(lambda x: x[0])\n",
    "df_first_clean['Subjectivity'] = df_first_clean['first_clean'].apply(sentiment).apply(lambda x: x[1])\n",
    "\n",
    "df_first_clean.to_pickle('pickled_data/data_first_clean_sentiment.pkl')\n",
    "\n",
    "df_first_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis based on economic growth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection of years of economic crisis or growth:\n",
    "list_economic_crisis = [1930, 1931, 1932, 1986, 1987, 1988, 2008, 2009, 2010]\n",
    "list_economic_growth = [1926, 1927, 1928, 1992, 1993, 1994, 2005, 2006, 2007]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main method for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_financial_sentiment(speech):\n",
    "    #We remove '\\n':\n",
    "    speech = speech.replace('\\n', '')\n",
    "    \n",
    "    #We split the speech into sentences to optimize for nltk.sentiment:\n",
    "    list_sentences = speech.split(sep='. ')\n",
    "    \n",
    "    #Add all the polarity scores of the sentences to a list:\n",
    "    scores = []\n",
    "    \n",
    "    sentimentIntensityAnalyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for sentence in list_sentences:\n",
    "        scores.append(sentimentIntensityAnalyzer.polarity_scores(sentence))\n",
    "    \n",
    "    #Sums and finds average polarity for each speech:\n",
    "    list_all_posistive = []\n",
    "    list_all_neutral = []\n",
    "    list_all_negative = []\n",
    "    list_all_compound = []\n",
    "    \n",
    "    for score in scores:\n",
    "        for key, value in score.items():\n",
    "            if key == 'pos':\n",
    "                list_all_posistive.append(value)\n",
    "            if key == 'neu':\n",
    "                list_all_neutral.append(value)\n",
    "            if key == 'neg':\n",
    "                list_all_negative.append(value)\n",
    "            if key == 'compound':\n",
    "                list_all_compound.append(value)\n",
    "                \n",
    "    avg_posistive = round(sum(list_all_posistive) / len(list_all_posistive), 4)\n",
    "    avg_neutral = round(sum(list_all_neutral) / len(list_all_neutral), 4)\n",
    "    avg_negative = round(sum(list_all_negative) / len(list_all_negative), 4)\n",
    "    avg_compound = round(sum(list_all_compound) / len(list_all_compound), 4)\n",
    "     \n",
    "    #Result is a dictionary of the same format as from the original 'SentimentIntensityAnalyzer..polarity_scores()' method:\n",
    "    result = {'neg': avg_negative, 'neu': avg_neutral, 'pos': avg_posistive, 'compound': avg_compound}\n",
    "    \n",
    "    #Return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple sentences:\n",
    "positive_sentence_1 = 'I love CPHBusiness and I think it is the best school'\n",
    "negative_sentence_1 = 'I hate CPHBusienss and I think it is the worst school'\n",
    "\n",
    "\n",
    "print('SIMPLE:')\n",
    "print('Positive:')\n",
    "print(method_financial_sentiment(positive_sentence_1))\n",
    "print('Negative:')\n",
    "print(method_financial_sentiment(negative_sentence_1))\n",
    "\n",
    "\n",
    "#Complex sentences (from 2002 speech):\n",
    "positive_sentence_2 = 'America and Afghanistan are now allies against terror. We will be partners in rebuilding that country. And this evening we welcome the distinguished interim leader of a liberated Afghanistan: Chairman Hamid Karzai. The last time we met in this chamber, the mothers and daughters of Afghanistan were captives in their own homes, forbidden from working or going to school. Today women are free, and are part of Afghanistans new government. And we welcome the new Minister of Womens Affairs, Doctor Sima Samar.'\n",
    "negative_sentence_2 = 'What we have found in Afghanistan confirms that, far from ending there, our war against terror is only beginning. Most of the 19 men who hijacked planes on September the 11th were trained in Afghanistans camps, and so were tens of thousands of others.  Thousands of dangerous killers, schooled in the methods of murder, often supported by outlaw regimes, are now spread throughout the world like ticking time bombs, set to go off without warning. '\n",
    "\n",
    "print('\\nCOMPLEX:')\n",
    "print('(Slightly) Positive:')\n",
    "print(method_financial_sentiment(positive_sentence_2))\n",
    "print('Negative:')\n",
    "print(method_financial_sentiment(negative_sentence_2))\n",
    "\n",
    "\n",
    "#Before and after 9/11:\n",
    "speech_2001 = df_second_clean['speech'][100]\n",
    "speech_2002 = df_second_clean['speech'][101]\n",
    "\n",
    "print('\\nBEFORE AND AFTER 9/11:')\n",
    "print('Before (2001):')\n",
    "print(method_financial_sentiment(speech_2001))\n",
    "print('After (2002):')\n",
    "print(method_financial_sentiment(speech_2002))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis using the main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes dataframe and removes unused column (if exists):\n",
    "df_economic_sentiment = df_second_clean.copy()\n",
    "if 'second_clean_lemmatized' in df_economic_sentiment.columns:\n",
    "    del df_economic_sentiment['second_clean_lemmatized']\n",
    "    \n",
    "# Uses lists created above to select years of crisis and growth:\n",
    "df_crisis = df_economic_sentiment.loc[df_economic_sentiment['year'].isin(list_economic_crisis), ['speech']]\n",
    "df_growth = df_economic_sentiment.loc[df_economic_sentiment['year'].isin(list_economic_growth), ['speech']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result A: We get one combined result for crisis and growth periods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines all speeches from the selected years of crisis and growth:\n",
    "crisis_speeches = ''.join(df_crisis.speech)\n",
    "growth_speeches = ''.join(df_growth.speech)\n",
    "\n",
    "# Uses main method to perform sentiment analysis:\n",
    "dict_crisis_result = method_financial_sentiment(crisis_speeches)\n",
    "dict_growth_result = method_financial_sentiment(growth_speeches)\n",
    "\n",
    "# Print result (plot doesn't make sense here):\n",
    "print('CRISIS YEARS:')\n",
    "print(dict_crisis_result)\n",
    "print('\\nGROWTH YEARS:')\n",
    "print(dict_growth_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result B: Results for each speech is added to DataFrame (for potential further analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds result to main DataFrame:\n",
    "df_economic_sentiment['sentiment_crisis'] = df_crisis.speech.apply(method_financial_sentiment)\n",
    "df_economic_sentiment['sentiment_growth'] = df_growth.speech.apply(method_financial_sentiment)\n",
    "\n",
    "# Removes 'speech' column (if exists):\n",
    "if 'speech' in df_economic_sentiment.columns:\n",
    "    del df_economic_sentiment['speech']\n",
    "\n",
    "# Removes all years from DataFrame that is not part of the analysis:\n",
    "df_economic_sentiment = df_economic_sentiment.dropna(subset=['sentiment_crisis', 'sentiment_growth'], thresh=1)\n",
    "\n",
    "#Print result:\n",
    "df_economic_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text complexity (LIX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of LIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Calculation:</b>\n",
    "\n",
    "<img src=\"./images/lix.png\" align=left>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<p>where</p>\n",
    "\n",
    "<ul>\n",
    "<li>A is the number of words,</li>\n",
    "<li>B is the number of periods (defined by period or colon), and</li>\n",
    "<li>C is the number of long words (more than 6 letters).</li>\n",
    "</ul>\n",
    "<p><i>Source: Wikipedia</i><p>\n",
    "<br>\n",
    "\n",
    "<b>Scores:</b>\n",
    "<ul>\n",
    "    <li>Greater than 54: Very difficult</li>\n",
    "    <li>Between 45-54 Difficult</li>\n",
    "    <li>Between 35-44 Medium</li>\n",
    "    <li>Between 25-34 Easy</li>\n",
    "    <li>Less than 25: Very easy</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIX analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy main dataframe (first clean - we don't want lemmatization here!):\n",
    "df_lix = df_first_clean.copy()\n",
    "df_lix = df_lix.drop(columns = ['speech', 'first_clean_tokenized'])\n",
    "\n",
    "\n",
    "# WORD COUNT:\n",
    "# Method to calculate total words in each speach:\n",
    "def word_count(speech):\n",
    "    return sum(1 for word in speech.split(' '))\n",
    "\n",
    "# Get total words in each speech and add result to DataFrame:\n",
    "df_lix['word_count'] = df_lix.first_clean.apply(word_count)\n",
    "\n",
    "\n",
    "# COUNT OF PERIODS:\n",
    "# Method to calculate total number of periods (period or colon) in each speach:\n",
    "def period_count(speech):\n",
    "    result = speech.count('.') + speech.count(':')\n",
    "    return result\n",
    "\n",
    "# Get total number of periods in each speech and add result to DataFrame:\n",
    "df_lix['period_count'] = df_lix.first_clean.apply(period_count)\n",
    "\n",
    "\n",
    "# COUNT OF LONG WORDS:\n",
    "# Method to long words (< 6 characters) in each speach:\n",
    "def long_word_count(speech):\n",
    "    count = 0\n",
    "    for word in speech.split(' '):\n",
    "        if len(word) > 6:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Get total number of long words in each speech and add result to DataFrame:\n",
    "df_lix['long_words_count'] = df_lix.first_clean.apply(long_word_count)\n",
    "\n",
    "\n",
    "# CALCULATE LIX:\n",
    "# Calculates LIX for each speech and adds result to DataFrame:\n",
    "df_lix['lix'] = (df_lix.word_count/df_lix.period_count) + ((df_lix.long_words_count*100) / df_lix.word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present results from LIX analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development in LIX over the years for all speeches:\n",
    "df_lix.plot(x = 'year', y = 'lix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development in LIX showing difference between Democrats and Republicans:\n",
    "df_lix_party = df_lix.copy()\n",
    "\n",
    "df_lix_party = df_lix.pivot(index='year', columns='Party', values='lix')\n",
    "df_lix_party.plot(color=['blue', 'red'])\n",
    "\n",
    "plt.ylabel('LIX')\n",
    "plt.xlabel('YEARS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of between speeches performed by Obama and Trump:\n",
    "mask_obama = (df_lix['President'] == 'Barack Obama')\n",
    "mask_trump = (df_lix['President'] == 'Donald Trump')\n",
    "\n",
    "avg_lix_obama = df_lix[mask_obama]['lix'].mean()\n",
    "avg_lix_trump = df_lix[mask_trump]['lix'].mean()\n",
    "\n",
    "x = [\"Obama\", \"Trump\"]\n",
    "y = [avg_lix_obama, avg_lix_trump]\n",
    "\n",
    "\n",
    "plt.bar(x,y)\n",
    "plt.ylabel('AVG. LIX')\n",
    "plt.xlabel('PRESIDENT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Word frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masks to filter on democrats and republicans\n",
    "mask_democrats = (df_first_clean['Party'] == 'Democrat')\n",
    "mask_republicans = (df_first_clean['Party'] == 'Republican')\n",
    "\n",
    "#DEMOCRATS:\n",
    "#Filter dataframe on democrats:\n",
    "df_first_clean_democrats = df_first_clean[mask_democrats]\n",
    "#Takes 'first_clean_tokenized' column and converts to list:\n",
    "tokenized_speeches_democrats = df_first_clean_democrats['first_clean_tokenized'].tolist()\n",
    "#Combines list of lists into one list:\n",
    "tokenized_speeches_democrats = list(itertools.chain.from_iterable(tokenized_speeches_democrats))\n",
    "\n",
    "#Republicans:\n",
    "#Filter dataframe on republicans:\n",
    "df_first_clean_republicans = df_first_clean[mask_republicans]\n",
    "#Takes 'first_clean_tokenized' column and converts to list:\n",
    "tokenized_speeches_republicans = df_first_clean_republicans['first_clean_tokenized'].tolist()\n",
    "#Combines list of lists into one list:\n",
    "tokenized_speeches_republicans = list(itertools.chain.from_iterable(tokenized_speeches_republicans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks word freq in all democrat speeches:\n",
    "freqDist_democrats = FreqDist(tokenized_speeches_democrats)\n",
    "\n",
    "dem_to_dict = dict(freqDist_democrats.most_common(20))\n",
    "\n",
    "    \n",
    "plt.bar(*zip(*dem_to_dict.items()), color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('D - 20 most frequent words')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "freqDist_democrats.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks word freq in all republican speeches:\n",
    "freqDist_republicans = FreqDist(tokenized_speeches_republicans)\n",
    "\n",
    "rep_to_dict = dict(freqDist_republicans.most_common(20))\n",
    "\n",
    "    \n",
    "plt.bar(*zip(*rep_to_dict.items()), color='red')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('D - 20 most frequent words')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "freqDist_republicans.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masks to filter on democrats and republicans\n",
    "mask_democrats = (df_second_clean['Party'] == 'Democrat')\n",
    "mask_republicans = (df_second_clean['Party'] == 'Republican')\n",
    "\n",
    "#DEMOCRATS:\n",
    "#Filter dataframe on democrats:\n",
    "df_second_clean_democrats = df_second_clean[mask_democrats]\n",
    "#Takes 'second_clean_lemmatized' column and converts to list:\n",
    "lemmatized_speeches_democrats = df_second_clean_democrats['second_clean_lemmatized'].tolist()\n",
    "#Combines list of lists into one list:\n",
    "lemmatized_speeches_democrats = list(itertools.chain.from_iterable(lemmatized_speeches_democrats))\n",
    "\n",
    "#Republicans:\n",
    "#Filter dataframe on republicans:\n",
    "df_second_clean_republicans = df_second_clean[mask_republicans]\n",
    "#Takes 'second_clean_lemmatized' column and converts to list:\n",
    "lemmatized_speeches_republicans = df_second_clean_republicans['second_clean_lemmatized'].tolist()\n",
    "#Combines list of lists into one list:\n",
    "lemmatized_speeches_republicans = list(itertools.chain.from_iterable(lemmatized_speeches_republicans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks word freq in all democrat speeches:\n",
    "freqDistr_democrats = FreqDist(lemmatized_speeches_democrats)\n",
    "\n",
    "dems_to_dict = dict(freqDistr_democrats.most_common(20))\n",
    "\n",
    "plt.bar(*zip(*dems_to_dict.items()), color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('D - 20 most frequent words')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "freqDist_democrats.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks word freq in all republican speeches:\n",
    "freqDistr_republicans = FreqDist(lemmatized_speeches_republicans)\n",
    "\n",
    "reps_to_dict = dict(freqDistr_republicans.most_common(20))\n",
    "\n",
    "plt.bar(*zip(*reps_to_dict.items()), color='red')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('D - 20 most frequent words')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "freqDist_republicans.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
